{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5476d61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !apt-get install poppler-utils tesseract-ocr libmagic-dev\n",
    "# %pip install -Uq \"unstructured[all-docs]\" pillow lxml pillow\n",
    "# %pip install -Uq chromadb tiktoken\n",
    "# %pip install -Uq langchain langchain-community langchain-ollama langchain-huggingface\n",
    "# %pip install -Uq python_dotenv sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64a627c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete! Make sure Ollama is installed and running.\n",
      "Required Ollama models:\n",
      "  ollama pull llama3.1:8b\n",
      "  ollama pull llava:7b\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Optional: LangChain tracing (still useful for debugging)\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = \"sk-...\"  # Only if you want tracing\n",
    "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "\n",
    "print(\"Setup complete! Make sure Ollama is installed and running.\")\n",
    "print(\"Required Ollama models:\")\n",
    "print(\"  ollama pull llama3.1:8b\")\n",
    "print(\"  ollama pull llava:7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b09cec36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arnabchakraborty/Desktop/WrishavSett/multimodal-rag/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/arnabchakraborty/Desktop/WrishavSett/multimodal-rag/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fetching 1 files: 100%|██████████| 1/1 [00:00<00:00, 3068.25it/s]\n",
      "The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Extract the data\n",
    "\n",
    "## Partition PDF tables, text and images\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "output_path = \"/Users/arnabchakraborty/Desktop/WrishavSett/multimodal-rag/\"\n",
    "file_path = output_path + 'NAGFORM_MANUAL.pdf'\n",
    "\n",
    "# Reference: https://docs.unstructured.io/open-source/core-functionality/chunking\n",
    "chunks = partition_pdf(\n",
    "    filename=file_path,\n",
    "    infer_table_structure=True,            # extract tables\n",
    "    strategy=\"hi_res\",                     # mandatory to infer tables\n",
    "\n",
    "    extract_image_block_types=[\"Image\"],   # Add 'Table' to list to extract image of tables\n",
    "    # image_output_dir_path=output_path,   # if None, images and tables will saved in base64\n",
    "\n",
    "    extract_image_block_to_payload=True,   # if true, will extract base64 for API usage\n",
    "\n",
    "    chunking_strategy=\"by_title\",          # or 'basic'\n",
    "    max_characters=10000,                  # defaults to 500\n",
    "    combine_text_under_n_chars=2000,       # defaults to 0\n",
    "    new_after_n_chars=6000,\n",
    "\n",
    "    # extract_images_in_pdf=True,          # deprecated\n",
    ")\n",
    "\n",
    "# We get 2 types of elements from the partition_pdf function\n",
    "set([str(type(el)) for el in chunks])\n",
    "\n",
    "# Each CompositeElement containes a bunch of related elements.\n",
    "# This makes it easy to use these elements together in a RAG pipeline.\n",
    "\n",
    "chunks[3].metadata.orig_elements\n",
    "\n",
    "## Separate extracted elements into tables, text and images\n",
    "\n",
    "# separate tables from texts\n",
    "tables = []\n",
    "texts = []\n",
    "\n",
    "for chunk in chunks:\n",
    "    if \"Table\" in str(type(chunk)):\n",
    "        tables.append(chunk)\n",
    "\n",
    "    if \"CompositeElement\" in str(type((chunk))):\n",
    "        texts.append(chunk)\n",
    "\n",
    "# Get the images from the CompositeElement objects\n",
    "def get_images_base64(chunks):\n",
    "    images_b64 = []\n",
    "    for chunk in chunks:\n",
    "        if \"CompositeElement\" in str(type(chunk)):\n",
    "            chunk_els = chunk.metadata.orig_elements\n",
    "            for el in chunk_els:\n",
    "                if \"Image\" in str(type(el)):\n",
    "                    images_b64.append(el.metadata.image_base64)\n",
    "    return images_b64\n",
    "\n",
    "images = get_images_base64(chunks)\n",
    "\n",
    "## Check what the images look like\n",
    "\n",
    "import base64\n",
    "from IPython.display import Image, display\n",
    "\n",
    "def display_base64_image(base64_code):\n",
    "    # Decode the base64 string to binary\n",
    "    image_data = base64.b64decode(base64_code)\n",
    "    # Display the image\n",
    "    display(Image(data=image_data))\n",
    "\n",
    "display_base64_image(images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3895d3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing text chunks...\n",
      "Processed 2/71 text chunks\n",
      "Processed 4/71 text chunks\n",
      "Processed 6/71 text chunks\n",
      "Processed 8/71 text chunks\n",
      "Processed 10/71 text chunks\n",
      "Processed 12/71 text chunks\n",
      "Processed 14/71 text chunks\n",
      "Processed 16/71 text chunks\n",
      "Processed 18/71 text chunks\n",
      "Processed 20/71 text chunks\n",
      "Processed 22/71 text chunks\n",
      "Processed 24/71 text chunks\n",
      "Processed 26/71 text chunks\n",
      "Processed 28/71 text chunks\n",
      "Processed 30/71 text chunks\n",
      "Processed 32/71 text chunks\n",
      "Processed 34/71 text chunks\n",
      "Processed 36/71 text chunks\n",
      "Processed 38/71 text chunks\n",
      "Processed 40/71 text chunks\n",
      "Processed 42/71 text chunks\n",
      "Processed 44/71 text chunks\n",
      "Processed 46/71 text chunks\n",
      "Processed 48/71 text chunks\n",
      "Processed 50/71 text chunks\n",
      "Processed 52/71 text chunks\n",
      "Processed 54/71 text chunks\n",
      "Processed 56/71 text chunks\n",
      "Processed 58/71 text chunks\n",
      "Processed 60/71 text chunks\n",
      "Processed 62/71 text chunks\n",
      "Processed 64/71 text chunks\n",
      "Processed 66/71 text chunks\n",
      "Processed 68/71 text chunks\n",
      "Processed 70/71 text chunks\n",
      "Processed 71/71 text chunks\n",
      "Summarizing images...\n",
      "Processed image 1/171\n",
      "Processed image 2/171\n",
      "Processed image 3/171\n",
      "Processed image 4/171\n",
      "Processed image 5/171\n",
      "Processed image 6/171\n",
      "Processed image 7/171\n",
      "Processed image 8/171\n",
      "Processed image 9/171\n",
      "Processed image 10/171\n",
      "Processed image 11/171\n",
      "Processed image 12/171\n",
      "Processed image 13/171\n",
      "Processed image 14/171\n",
      "Processed image 15/171\n",
      "Processed image 16/171\n",
      "Processed image 17/171\n",
      "Processed image 18/171\n",
      "Processed image 19/171\n",
      "Processed image 20/171\n",
      "Processed image 21/171\n",
      "Processed image 22/171\n",
      "Processed image 23/171\n",
      "Processed image 24/171\n",
      "Processed image 25/171\n",
      "Processed image 26/171\n",
      "Processed image 27/171\n",
      "Processed image 28/171\n",
      "Processed image 29/171\n",
      "Processed image 30/171\n",
      "Processed image 31/171\n",
      "Processed image 32/171\n",
      "Processed image 33/171\n",
      "Processed image 34/171\n",
      "Processed image 35/171\n",
      "Processed image 36/171\n",
      "Processed image 37/171\n",
      "Processed image 38/171\n",
      "Processed image 39/171\n",
      "Processed image 40/171\n",
      "Processed image 41/171\n",
      "Processed image 42/171\n",
      "Processed image 43/171\n",
      "Processed image 44/171\n",
      "Processed image 45/171\n",
      "Processed image 46/171\n",
      "Processed image 47/171\n",
      "Processed image 48/171\n",
      "Processed image 49/171\n",
      "Processed image 50/171\n",
      "Processed image 51/171\n",
      "Processed image 52/171\n",
      "Processed image 53/171\n",
      "Processed image 54/171\n",
      "Processed image 55/171\n",
      "Processed image 56/171\n",
      "Processed image 57/171\n",
      "Processed image 58/171\n",
      "Processed image 59/171\n",
      "Processed image 60/171\n",
      "Processed image 61/171\n",
      "Processed image 62/171\n",
      "Processed image 63/171\n",
      "Processed image 64/171\n",
      "Processed image 65/171\n",
      "Processed image 66/171\n",
      "Processed image 67/171\n",
      "Processed image 68/171\n",
      "Processed image 69/171\n",
      "Processed image 70/171\n",
      "Processed image 71/171\n",
      "Processed image 72/171\n",
      "Processed image 73/171\n",
      "Processed image 74/171\n",
      "Processed image 75/171\n",
      "Processed image 76/171\n",
      "Processed image 77/171\n",
      "Processed image 78/171\n",
      "Processed image 79/171\n",
      "Processed image 80/171\n",
      "Processed image 81/171\n",
      "Processed image 82/171\n",
      "Processed image 83/171\n",
      "Processed image 84/171\n",
      "Processed image 85/171\n",
      "Processed image 86/171\n",
      "Processed image 87/171\n",
      "Processed image 88/171\n",
      "Processed image 89/171\n",
      "Processed image 90/171\n",
      "Processed image 91/171\n",
      "Processed image 92/171\n",
      "Processed image 93/171\n",
      "Processed image 94/171\n",
      "Processed image 95/171\n",
      "Processed image 96/171\n",
      "Processed image 97/171\n",
      "Processed image 98/171\n",
      "Processed image 99/171\n",
      "Processed image 100/171\n",
      "Processed image 101/171\n",
      "Processed image 102/171\n",
      "Processed image 103/171\n",
      "Processed image 104/171\n",
      "Processed image 105/171\n",
      "Processed image 106/171\n",
      "Processed image 107/171\n",
      "Processed image 108/171\n",
      "Processed image 109/171\n",
      "Processed image 110/171\n",
      "Processed image 111/171\n",
      "Processed image 112/171\n",
      "Processed image 113/171\n",
      "Processed image 114/171\n",
      "Processed image 115/171\n",
      "Processed image 116/171\n",
      "Processed image 117/171\n",
      "Processed image 118/171\n",
      "Processed image 119/171\n",
      "Processed image 120/171\n",
      "Processed image 121/171\n",
      "Processed image 122/171\n",
      "Processed image 123/171\n",
      "Processed image 124/171\n",
      "Processed image 125/171\n",
      "Processed image 126/171\n",
      "Processed image 127/171\n",
      "Processed image 128/171\n",
      "Processed image 129/171\n",
      "Processed image 130/171\n",
      "Processed image 131/171\n",
      "Processed image 132/171\n",
      "Processed image 133/171\n",
      "Processed image 134/171\n",
      "Processed image 135/171\n",
      "Processed image 136/171\n",
      "Processed image 137/171\n",
      "Processed image 138/171\n",
      "Processed image 139/171\n",
      "Processed image 140/171\n",
      "Processed image 141/171\n",
      "Processed image 142/171\n",
      "Processed image 143/171\n",
      "Processed image 144/171\n",
      "Processed image 145/171\n",
      "Processed image 146/171\n",
      "Processed image 147/171\n",
      "Processed image 148/171\n",
      "Processed image 149/171\n",
      "Processed image 150/171\n",
      "Processed image 151/171\n",
      "Processed image 152/171\n",
      "Processed image 153/171\n",
      "Processed image 154/171\n",
      "Processed image 155/171\n",
      "Processed image 156/171\n",
      "Processed image 157/171\n",
      "Processed image 158/171\n",
      "Processed image 159/171\n",
      "Processed image 160/171\n",
      "Processed image 161/171\n",
      "Processed image 162/171\n",
      "Processed image 163/171\n",
      "Processed image 164/171\n",
      "Processed image 165/171\n",
      "Processed image 166/171\n",
      "Processed image 167/171\n",
      "Processed image 168/171\n",
      "Processed image 169/171\n",
      "Processed image 170/171\n",
      "Processed image 171/171\n",
      "All summarization complete!\n"
     ]
    }
   ],
   "source": [
    "### Summarize data\n",
    "\n",
    "## Text and table summaries using Ollama\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Prompt for text and table summarization\n",
    "prompt_text = \"\"\"\n",
    "You are an assistant tasked with summarizing text.\n",
    "Give a concise summary of the text.\n",
    "\n",
    "Respond only with the summary, no additional comment.\n",
    "Do not start your message by saying \"Here is a summary\" or anything like that.\n",
    "Just give the summary as it is.\n",
    "\n",
    "Text chunk: {element}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "\n",
    "# Summary chain using local Ollama Llama model\n",
    "model = ChatOllama(\n",
    "    model=\"llama3.1:8b\",\n",
    "    temperature=0.5,\n",
    "    # Ollama runs locally, so we can set reasonable timeouts\n",
    "    request_timeout=60.0\n",
    ")\n",
    "\n",
    "summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()\n",
    "\n",
    "print(\"Summarizing text chunks...\")\n",
    "# Process texts in smaller batches to avoid overwhelming local model\n",
    "batch_size = 2\n",
    "text_summaries = []\n",
    "for i in range(0, len(texts), batch_size):\n",
    "    batch = texts[i:i+batch_size]\n",
    "    batch_summaries = summarize_chain.batch(batch)\n",
    "    text_summaries.extend(batch_summaries)\n",
    "    print(f\"Processed {min(i+batch_size, len(texts))}/{len(texts)} text chunks\")\n",
    "\n",
    "# print(\"Summarizing table chunks...\")\n",
    "# # Summarize tables\n",
    "# tables_html = [table.metadata.text_as_html for table in tables]\n",
    "# table_summaries = []\n",
    "# for i in range(0, len(tables_html), batch_size):\n",
    "#     batch = tables_html[i:i+batch_size]\n",
    "#     batch_summaries = summarize_chain.batch(batch)\n",
    "#     table_summaries.extend(batch_summaries)\n",
    "#     print(f\"Processed {min(i+batch_size, len(tables_html))}/{len(tables_html)} table chunks\")\n",
    "\n",
    "## Image summaries using Ollama LLaVA\n",
    "\n",
    "# Prompt for image description\n",
    "prompt_template = \"\"\"Describe this image in detail. \n",
    "For context, this image is part of a user manual.\n",
    "Be specific about the content you see.\n",
    "Focus on the technical details and the overall functionality that would be useful for understanding this image.\"\"\"\n",
    "\n",
    "print(\"Summarizing images...\")\n",
    "# LLaVA model for vision tasks\n",
    "vision_model = ChatOllama(\n",
    "    model=\"llava:7b\",\n",
    "    temperature=0.3,  # Lower temperature for more consistent descriptions\n",
    "    request_timeout=120.0  # Vision tasks may take longer\n",
    ")\n",
    "\n",
    "# Process images one by one (vision models are more resource intensive)\n",
    "image_summaries = []\n",
    "for i, image_b64 in enumerate(images):\n",
    "    try:\n",
    "        # Create messages for vision model\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": prompt_template,\n",
    "                \"images\": [image_b64]  # LLaVA expects images in this format\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Get summary for this image\n",
    "        summary = vision_model.invoke(messages)\n",
    "        if hasattr(summary, 'content'):\n",
    "            image_summaries.append(summary.content)\n",
    "        else:\n",
    "            image_summaries.append(str(summary))\n",
    "        \n",
    "        print(f\"Processed image {i+1}/{len(images)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {i+1}: {str(e)}\")\n",
    "        image_summaries.append(f\"Error processing image: {str(e)}\")\n",
    "\n",
    "print(\"All summarization complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f564d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model...\n",
      "Creating vector store with disk persistence...\n",
      "Adding text summaries to vector store...\n",
      "✓ Added 71 text summaries\n",
      "Adding image summaries to vector store...\n",
      "✓ Added 171 image summaries\n",
      "Persisting vector store to disk...\n",
      "✓ Vector store saved to: /Users/arnabchakraborty/Desktop/WrishavSett/multimodal-rag/chroma_db\n",
      "Vector store setup complete!\n"
     ]
    }
   ],
   "source": [
    "### Load data and summaries to vectorstore\n",
    "\n",
    "## Create vector store with free embeddings\n",
    "\n",
    "import uuid\n",
    "import os\n",
    "import shutil\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.storage import InMemoryStore, LocalFileStore\n",
    "from langchain.schema.document import Document\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "\n",
    "def convert_element_to_document(element, content_type, index):\n",
    "    \"\"\"Convert unstructured elements to proper Document objects\"\"\"\n",
    "    try:\n",
    "        # Extract text content\n",
    "        if hasattr(element, 'text'):\n",
    "            content = str(element.text)\n",
    "        elif hasattr(element, 'metadata') and hasattr(element.metadata, 'text_as_html'):\n",
    "            content = str(element.metadata.text_as_html)\n",
    "        else:\n",
    "            content = str(element)\n",
    "        \n",
    "        # Create clean metadata dictionary\n",
    "        metadata = {\n",
    "            \"content_type\": content_type,\n",
    "            \"index\": index,\n",
    "            \"source\": f\"original_{content_type}\"\n",
    "        }\n",
    "        \n",
    "        # Add specific metadata if available\n",
    "        if hasattr(element, 'metadata'):\n",
    "            element_meta = element.metadata\n",
    "            if hasattr(element_meta, 'page_number'):\n",
    "                metadata[\"page_number\"] = element_meta.page_number\n",
    "            if hasattr(element_meta, 'filename'):\n",
    "                metadata[\"filename\"] = str(element_meta.filename)\n",
    "        \n",
    "        return Document(page_content=content, metadata=metadata)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not convert element {index} to document: {e}\")\n",
    "        return Document(\n",
    "            page_content=str(element), \n",
    "            metadata={\n",
    "                \"content_type\": content_type,\n",
    "                \"index\": index, \n",
    "                \"source\": f\"fallback_{content_type}\"\n",
    "            }\n",
    "        )\n",
    "\n",
    "# Setup persistent storage\n",
    "chroma_path = f\"{output_path}chroma_db/\"\n",
    "if os.path.exists(chroma_path):\n",
    "    print(f\"Removing existing ChromaDB directory: {chroma_path}\")\n",
    "    shutil.rmtree(chroma_path)\n",
    "\n",
    "# import tempfile\n",
    "\n",
    "# # Create a temporary, writable directory for Chroma\n",
    "# chroma_path = tempfile.mkdtemp(prefix=\"chroma_db_\")\n",
    "# print(f\"Using temporary Chroma DB directory at: {chroma_path}\")\n",
    "\n",
    "\n",
    "print(\"Loading embedding model...\")\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'mps'},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "print(\"Creating vector store with disk persistence...\")\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"multi_modal_rag\", \n",
    "    embedding_function=embedding_model,\n",
    "    persist_directory=\"./chroma_db/\"\n",
    ")\n",
    "\n",
    "# The storage layer for the parent documents\n",
    "# store = InMemoryStore()\n",
    "docstore_path = f\"{output_path}docstore/\"\n",
    "store = InMemoryStore()\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# The retriever\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "\n",
    "## Load the summaries and link them to the original data\n",
    "\n",
    "print(\"Adding text summaries to vector store...\")\n",
    "try:\n",
    "    if text_summaries and texts:\n",
    "        doc_ids = [str(uuid.uuid4()) for _ in texts]\n",
    "        \n",
    "        # Create summary documents for vectorstore\n",
    "        summary_texts = [\n",
    "            Document(\n",
    "                page_content=str(summary),\n",
    "                metadata={\n",
    "                    id_key: doc_ids[i],\n",
    "                    \"content_type\": \"text\",\n",
    "                    \"chunk_index\": i,\n",
    "                    \"source\": \"text_summary\"\n",
    "                }\n",
    "            ) for i, summary in enumerate(text_summaries) if summary and str(summary).strip()\n",
    "        ]\n",
    "        \n",
    "        # Convert original text elements to proper Document objects\n",
    "        original_text_docs = [\n",
    "            convert_element_to_document(text_element, \"text\", i) \n",
    "            for i, text_element in enumerate(texts)\n",
    "        ]\n",
    "        \n",
    "        if summary_texts:\n",
    "            retriever.vectorstore.add_documents(summary_texts)\n",
    "            retriever.docstore.mset(list(zip(doc_ids, original_text_docs)))\n",
    "            print(f\"✓ Added {len(summary_texts)} text summaries\")\n",
    "        else:\n",
    "            print(\"No valid text summaries to add\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"Error adding text summaries: {e}\")\n",
    "\n",
    "# print(\"Adding table summaries to vector store...\")\n",
    "# try:\n",
    "#     if table_summaries and tables:\n",
    "#         table_ids = [str(uuid.uuid4()) for _ in tables]\n",
    "        \n",
    "#         # Create summary documents for vectorstore\n",
    "#         summary_tables = [\n",
    "#             Document(\n",
    "#                 page_content=str(summary),\n",
    "#                 metadata={\n",
    "#                     id_key: table_ids[i],\n",
    "#                     \"content_type\": \"table\",\n",
    "#                     \"table_index\": i,\n",
    "#                     \"source\": \"table_summary\"\n",
    "#                 }\n",
    "#             ) for i, summary in enumerate(table_summaries) if summary and str(summary).strip()\n",
    "#         ]\n",
    "        \n",
    "#         # Convert original table elements to proper Document objects\n",
    "#         original_table_docs = [\n",
    "#             convert_element_to_document(table_element, \"table\", i)\n",
    "#             for i, table_element in enumerate(tables)\n",
    "#         ]\n",
    "        \n",
    "#         if summary_tables:\n",
    "#             retriever.vectorstore.add_documents(summary_tables)\n",
    "#             retriever.docstore.mset(list(zip(table_ids, original_table_docs)))\n",
    "#             print(f\"✓ Added {len(summary_tables)} table summaries\")\n",
    "#         else:\n",
    "#             print(\"No valid table summaries to add\")\n",
    "            \n",
    "# except Exception as e:\n",
    "#     print(f\"Error adding table summaries: {e}\")\n",
    "\n",
    "print(\"Adding image summaries to vector store...\")\n",
    "try:\n",
    "    if image_summaries and images:\n",
    "        img_ids = [str(uuid.uuid4()) for _ in images]\n",
    "        \n",
    "        # Create summary documents for vectorstore  \n",
    "        summary_img = [\n",
    "            Document(\n",
    "                page_content=str(summary),\n",
    "                metadata={\n",
    "                    id_key: img_ids[i],\n",
    "                    \"content_type\": \"image\",\n",
    "                    \"image_index\": i,\n",
    "                    \"source\": \"image_summary\"\n",
    "                }\n",
    "            ) for i, summary in enumerate(image_summaries) if summary and str(summary).strip()\n",
    "        ]\n",
    "        \n",
    "        # Create original image documents with base64 data\n",
    "        original_img_docs = [\n",
    "            Document(\n",
    "                page_content=f\"Image {i+1} from research paper (base64 encoded)\",\n",
    "                metadata={\n",
    "                    \"content_type\": \"image\",\n",
    "                    \"image_index\": i,\n",
    "                    \"image_base64\": img_b64,\n",
    "                    \"source\": \"original_image\"\n",
    "                }\n",
    "            ) for i, img_b64 in enumerate(images)\n",
    "        ]\n",
    "        \n",
    "        if summary_img:\n",
    "            retriever.vectorstore.add_documents(summary_img)\n",
    "            retriever.docstore.mset(list(zip(img_ids, original_img_docs)))\n",
    "            print(f\"✓ Added {len(summary_img)} image summaries\")\n",
    "        else:\n",
    "            print(\"No valid image summaries to add\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"Error adding image summaries: {e}\")\n",
    "\n",
    "print(\"Persisting vector store to disk...\")\n",
    "print(f\"✓ Vector store saved to: {os.path.abspath(chroma_path)}\")\n",
    "print(\"Vector store setup complete!\")\n",
    "\n",
    "## Test retrieval with robust error handling\n",
    "\n",
    "def safe_display_document(doc, index):\n",
    "    \"\"\"Safely display document information regardless of type\"\"\"\n",
    "    print(f\"\\n--- Document {index+1} ---\")\n",
    "    \n",
    "    try:\n",
    "        if hasattr(doc, 'page_content') and hasattr(doc, 'metadata'):\n",
    "            # It's a proper Document object\n",
    "            content_type = doc.metadata.get('content_type', 'unknown')\n",
    "            source = doc.metadata.get('source', 'unknown')\n",
    "            print(f\"Type: {content_type} | Source: {source}\")\n",
    "            print(f\"Content: {doc.page_content[:300]}...\")\n",
    "            \n",
    "            # Special handling for images\n",
    "            if content_type == \"image\" and \"image_base64\" in doc.metadata:\n",
    "                print(f\"[Contains base64 image data - {len(doc.metadata['image_base64'])} characters]\")\n",
    "                \n",
    "        elif hasattr(doc, 'page_content'):\n",
    "            # Document object without proper metadata\n",
    "            print(\"Type: Document (no metadata)\")\n",
    "            print(f\"Content: {doc.page_content[:300]}...\")\n",
    "            \n",
    "        else:\n",
    "            # Some other object type\n",
    "            print(f\"Type: {type(doc).__name__}\")\n",
    "            print(f\"Content: {str(doc)[:300]}...\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error displaying document: {e}\")\n",
    "        print(f\"Raw content: {str(doc)[:200]}...\")\n",
    "    \n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb334d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 4 documents:\n",
      "\n",
      "--- Document 1 ---\n",
      "Type: text | Source: original_text\n",
      "Content: 6.2. Menu Selection ‘Getting Too Many Designs’\n",
      "\n",
      "When this selection is made, the following dialog box comes up.\n",
      "\n",
      "- Limits On Blank Diamter : Rules Past ID: | @ Reset to before chan From: [5.000000 To: fo.oo000 EWE DEr Tighten Design Rules by: [o.oo x Save changed rules > Limits On Number of Operatio...\n",
      "------------------------------------------------------------\n",
      "\n",
      "--- Document 2 ---\n",
      "Type: text | Source: original_text\n",
      "Content: 6. Advanced Design Helper\n",
      "\n",
      "___________________________________________________________________________________\n",
      "\n",
      "In NAGFORM, the main menu has a separate title ‘Design Helper’ as shown below.\n",
      "\n",
      "NAGFORM: Program for Metal Forming Analysis and Design File Edit Model Material | Desien Helper Machine Dime...\n",
      "------------------------------------------------------------\n",
      "\n",
      "--- Document 3 ---\n",
      "Type: text | Source: original_text\n",
      "Content: 2.2.4 Help\n",
      "\n",
      "Help menu has three selections:\n",
      "\n",
      "• Version …\n",
      "\n",
      "• Unit System …\n",
      "\n",
      "• Help Topics …\n",
      "\n",
      "Read DXF File …\n",
      "\n",
      "•\n",
      "\n",
      "• Calculate …\n",
      "\n",
      "Copyright, Metal Forming Systems, Inc.\n",
      "\n",
      "10\n",
      "\n",
      "Version ... Unit System ... ad Help Topics . Read DXF File... Calculate » Weight... Weight... Surface Area. Heading: L/D Ratio......\n",
      "------------------------------------------------------------\n",
      "\n",
      "--- Document 4 ---\n",
      "Type: image | Source: original_image\n",
      "Content: Image 147 from research paper (base64 encoded)...\n",
      "[Contains base64 image data - 43368 characters]\n",
      "------------------------------------------------------------\n",
      "\n",
      "Testing additional queries...\n",
      "\n",
      "🔍 Query: 'attention mechanism'\n",
      "Found 4 results\n",
      "Preview (unknown): Image 20 from research paper (base64 encoded)...\n",
      "\n",
      "🔍 Query: 'transformer architecture'\n",
      "Found 4 results\n",
      "Preview (unknown): Image 61 from research paper (base64 encoded)...\n",
      "\n",
      "🔍 Query: 'experimental results'\n",
      "Found 4 results\n",
      "Preview (unknown): Image 62 from research paper (base64 encoded)...\n",
      "\n",
      "✅ Setup complete! Your multimodal RAG system is ready!\n",
      "📁 Vector database persisted at: /Users/arnabchakraborty/Desktop/WrishavSett/multimodal-rag/chroma_db\n",
      "📊 Total documents in store: 242\n"
     ]
    }
   ],
   "source": [
    "# print(\"\\nTesting retrieval...\")\n",
    "try:\n",
    "    docs = retriever.invoke(\"explain advanced design helper\")\n",
    "    print(f\"Retrieved {len(docs)} documents:\")\n",
    "    \n",
    "    for i, doc in enumerate(docs):\n",
    "        safe_display_document(doc, i)\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error during retrieval: {e}\")\n",
    "    print(\"Trying direct vectorstore search...\")\n",
    "    try:\n",
    "        direct_results = vectorstore.similarity_search(\"authors paper\", k=3)\n",
    "        print(f\"Direct search found {len(direct_results)} results:\")\n",
    "        for i, result in enumerate(direct_results):\n",
    "            safe_display_document(result, i)\n",
    "    except Exception as e2:\n",
    "        print(f\"Direct search also failed: {e2}\")\n",
    "\n",
    "## Test multiple queries\n",
    "print(\"\\nTesting additional queries...\")\n",
    "test_queries = [\n",
    "    \"attention mechanism\",\n",
    "    \"transformer architecture\", \n",
    "    \"experimental results\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    try:\n",
    "        print(f\"\\n🔍 Query: '{query}'\")\n",
    "        results = retriever.invoke(query)\n",
    "        print(f\"Found {len(results)} results\")\n",
    "        \n",
    "        if results:\n",
    "            first_result = results[0]\n",
    "            if hasattr(first_result, 'page_content'):\n",
    "                preview = first_result.page_content[:100]\n",
    "                content_type = getattr(first_result.metadata, 'content_type', 'unknown') if hasattr(first_result, 'metadata') else 'unknown'\n",
    "                print(f\"Preview ({content_type}): {preview}...\")\n",
    "            else:\n",
    "                print(f\"Preview: {str(first_result)[:100]}...\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error with query '{query}': {e}\")\n",
    "\n",
    "print(f\"\\n✅ Setup complete! Your multimodal RAG system is ready!\")\n",
    "print(f\"📁 Vector database persisted at: {os.path.abspath(chroma_path)}\")\n",
    "print(f\"📊 Total documents in store: {len(retriever.docstore.store) if hasattr(retriever.docstore, 'store') else 'Unknown'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5781915b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.9.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
